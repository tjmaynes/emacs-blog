#+TITLE: TJ Maynes
#+DESCRIPTION: Personal blog.

* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/test-driven-learning.org][Test-Driven Learning]]
:PROPERTIES:
:RSS_PERMALINK: test-driven-learning.html
:PUBDATE:  2020-01-24
:ID:       EC9863C9-B0A8-442A-8516-612C42F4C0EF
:END:
#+DESCRIPTION: In this blog post, I'll be discussing a strategy I use for rapidly learning a new programming language/library/tool that utilizes a test suite as a feedback tool.

I just wanted to discuss an approach I use to learn new programming languages, runtimes, libraries and/or implementing abstract concepts. The approach is essentially test-driven development, but instead of writing tests as a feedback mechanism for refactoring code, we are writing tests as a feedback mechanism for learning. Let's call this "Test-Driven Learning" for now. This test-driven learning approach gives us a quick feedback loop (via test-runner) and sandbox environment (via test-suite), that we can use for rapidly learning new concepts.

This feedback loop boils down to: *Hypothesize -> Test -> Evaluate -> Repeat*.

The feedback loop is similiar to using a REPL (Read-Evaluate-Print-Loop) to learning new concepts. However, I've found using most REPL environments (minus Emacs + Lisp setup) can feel too constraining to driven out building DSLs, learning new libraries, etc. Also, depending on the language/library/tool you are learning, there may not be a REPL available for you to learn with.

Ok, enough with the theory, let's jump in and learn through implementation.

** Implementing the Feedback Loop
   :PROPERTIES:
   :ID:       76D3F4CD-4C6F-4E6D-9578-626379893BD2
   :END:
To implement test-driven learning, you will need to get the language runtime on your machine. With the runtime on your machine, you'll need to acquire a testing framework to write your tests in.

#+BEGIN_QUOTE
*Tip*: If you are having trouble finding the "right" testing framework for your language of choice, try looking into the language's community. This research will enable you to learn more about the language and how it's community delivers software with the language. You can usually find these communities on different online forums like Google Groups, Slack, Twitter or with a quick [[https://duckduckgo.com/][Duck-Duck-Go]] search.
#+END_QUOTE

Assuming you have chosen a testing-framework, and you have the language runtime installed on your machine, your next step is fire up your favorite text-editor, or IDE, and write your test file. Depending on what you are trying to test, maybe it's a new language feature or a new third-party library, you'll want to expose that functionality to your test suite. Write a 
* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/a-functional-approach-error-handling.org][A Functional Approach: Error Handling]]
:PROPERTIES:
:RSS_PERMALINK: a-functional-approach-error-handling.html
:PUBDATE:  2020-01-15
:ID:       61326C36-9CF9-46E5-8945-CD96BB1E5E9F
:END:
#+DESCRIPTION: In this blog post, we'll be discussing some object-oriented error handling strategies that will iterate into the Either monad.

Recently, I've been asking myself, "How can this function fail?" This question has led to a different style of programming that I've chosen to utilize in various problem solving scenarios (mostly around integrations with external systems). Let's take a look at the following function written in [[https://kotlinlang.org/docs/reference/][Kotlin]]:

#+BEGIN_SRC kotlin
  fun add1(x: Int): Int = x + 1
#+END_SRC

This function is pretty straightforward, but let's say what it's intent is in a sentence: "Function add1 takes an Integer element 'x' and returns the Integer plus one." We can guarentee that every time this function is called with any integer value it will return the same result.

#+BEGIN_SRC kotlin
  assertEquals(1, add1(0))
  assertEquals(2, add1(1))
  assertEquals(3, add1(2))
  ...
  ...
#+END_SRC

From a functional programming perspective, this function would be considered [[https://en.wikipedia.org/wiki/Pure_functionXS][pure]]. A pure function is a function that has the same return value for the same input value and does not contain any side effects. The guarentees of a pure function enable software developers to write safer software. But what happens when you need to introduce a side effect into your application. Side effects are not necessarily a bad thing, it's actually great thing that they occur; it means your program is useful. Useful features of an application, may include (but not limited to):

User input
Talking to a database or external service
Receiving command-line arguments
Reading environment variables

While usefulness has its rewards, it also comes at a cost. Anytime we introduce a side effect into our software, we introduce additional ways for our application to fail. Let's take a look at what happens when we write a function that attempts to fetch a thing (T) from a database.

#+BEGIN_SRC kotlin
  fun <T> getById(id: String): T =
    databaseDriver.getById(id)
#+END_SRC

This simple function takes an id and returns a thing that is fetched from a database using a database driver. But, what happens when the thing is not in the database? If the thing is not in the database, then our function will not return our thing. Maybe we should rethink our approach to this function.

#+BEGIN_SRC kotlin
  fun <T> getById(id: String): T? =
    databaseDriver.getById(id) ?: null
#+END_SRC

There! That's it! We changed our return type to return an optional thing. This is convenient for us because that let's the caller of this function decide what to do when a thing is not found in our database.

But, what happens if we can't find our thing because we were able to connect to our database?

Let's assume our getById function provided by databaseDriver throws an exception. Unfortunately we have not managed what happens when our internal dependency throws an exception. Let's go through two different approaches to error handling throwable functions.

A common pattern for dealing with functions that throw, is to utilize the Try...catch pattern. As seen below...

#+BEGIN_SRC kotlin
  fun <T> getById(id: String): T? =
    try {
      databaseDriver.getById(id) ?: null
    } catch (e: Exception) {
      null
    }
#+END_SRC

This works pretty well for us. We've removed the "invisible"/implicit throwable from the function and are now back to returning a simple optional T value. However, this simplicity has come at a price.

With the above approach we have given the caller of this method a not so "truthy" return value. If an exception occurs, the caller of this method will not know, based on the name of the function, if the item was found or something else has happened (unable to connect to database, table not found, etc).

I think we can do better job giving more information back to the caller of this method. Let's rewrite the function using two callbacks: onSuccess and onFailure.

#+BEGIN_SRC kotlin
  fun <T> getById(id: String, onSuccess: (T?) -> Void, onFailure: (Exception) -> Void) {
    try {
      onSuccess(databaseDriver.getById ?: null)
    } catch (e: Exception) {
      onFailure(e)
    }
  }
#+END_SRC

The above function says "given an id, we could be in a success or failure state". The function signature tells us (without looking at the implementation code) that our getById method can succeed or fail for different reasons. While this code is functionally more correct, it is harder to use now since we are relying on callbacks.

Let's look at how this is harder to use now. Previously we were able to get our value in one line. As so...

#+BEGIN_SRC kotlin
  val result: AnObject? = getById("some-id")
  if (result != null) {
    // do something
  } else {
    // do something else
  }
#+END_SRC

The above style of using our getById function is easier to reason about since it following a conventional style of programming that most developers are used to. But with callbacks, we have to work with our function this way...

#+BEGIN_SRC kotlin
  getById("some-id", { result ->
    // do something
  }, { exception ->
    // do something else
  })
#+END_SRC

What if we had a type that could represent these types...

** The Either Monad
   :PROPERTIES:
   :ID:       84DF5593-FC46-45D4-A873-B230C6EBADD6
   :END:
The *Either* monad is a monadic data type that allows you to encapsulate a result that returns either one possible outcome or another possible outcome. Since Kotlin does not include an Either monad is it's standard library, we'll need to pull in a functional programming library called [[https://arrow-kt.io/docs/core/][Arrow]] for our example code.

#+BEGIN_SRC kotlin
  fun <T> getById(id: String): Either<RepositoryError, T> =
    try {
      val result = databaseDriver.getById(id)
      if (result) {
        Either.Right(result)
      }
      Either.Left(RepositoryError.NotFound)
    } catch (e: Exception) {
      Either.Left(RepositoryError.Unknown(e.localizedMessage))
    }
#+END_SRC

When we /map/, equivalently /bind/, over our list, we gain the ability to safely access the /Right/ results from our country variable. If we have any /Right/ values then we transform/apply the /capitalize/ method to all of our /Right/ values. If we have any /Left/ values then we will log exceptions.

#+BEGIN_SRC kotlin
  when(val result = getById("some-id")) {
    is Either.Right -> // do something
    is Either.Left -> // handle error
  }
#+END_SRC
* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/sql-murder-mystery.org][SQL Murder Mystery]]
:PROPERTIES:
:RSS_PERMALINK: sql-murder-mystery.html
:PUBDATE:  2019-12-16
:ID:       BA3A5C43-59DE-49A4-850C-AFF9177DC866
:END:
#+DESCRIPTION: In this blog post, I'll be talking briefly about a fun game I found via Hackernews called SQL Murder Mystery.

Normally when learning a new programming language, library, or concept, I'll fire up an editor and write a set of tests to drive out the knowledge that I am seeking. For a while now, I've wanted to learn more SQL but haven't made time to do so. Luckily this [[https://news.ycombinator.com/item?id=21799988][game]], called SQL Murder Mystery, showed up on Hackernews one cold winter morning.

[[https://mystery.knightlab.com/][SQL Murder Mystery]] utilizes your SQL querying abilities to solve a "whodunnit" murder mystery with only a single clue to start from. To solve these problems you'll need to write SQLite queries, which I have some decent CRUD knowledge of, as it is the SQL language of choice for the game. I had a lot of fun playing this game and wish more games like this were created for developers.

Below are the SQLite queries I used to solve the Murder Mystery game.

** Solutions
   :PROPERTIES:
   :ID:       03DC5599-BE5C-4906-A205-10422649A9F8
   :END:
*** Find Report query
    :PROPERTIES:
    :ID:       BC023CFE-BD23-4BA2-AAB8-C90F41D460D3
    :END:
#+BEGIN_SRC sql
  select
    crime_scene_report.description
  from crime_scene_report
  where
    date = 20180115
    and city = "SQL City"
    and type = "murder"
#+END_SRC

*Results*:
| description                                                                                                                                                                               |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Security footage shows that there were 2 witnesses. The first witness lives at the last house on "Northwestern Dr". The second witness, named Annabel, lives somewhere on "Franklin Ave". |

*** Find Morty Query
    :PROPERTIES:
    :ID:       829C23D2-3AF2-4895-B1C4-76B9B294AA6D
    :END:
#+BEGIN_SRC sql
  select
    person.name,
    interview.transcript
  from person
  inner join drivers_license on person.license_id=drivers_license.id
  inner join interview on person.id=interview.person_id
  where person.address_street_name = "Northwestern Dr"
  order by person.address_number desc
  limit 1
#+END_SRC

*Results*
| name           | transcript                                                                                                                                                                                                                      |
|----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Morty Schapiro | I heard a gunshot and then saw a man run out. He had a "Get Fit Now Gym" bag. The membership number on the bag started with "48Z". Only gold members have those bags. The man got into a car with a plate that included "H42W". |

*** Annabel query
    :PROPERTIES:
    :ID:       5DE66F98-325F-47A9-BA38-6C1E29B14F09
    :END:
#+BEGIN_SRC sql
  select
    person.name,
    interview.transcript
  from person
  inner join drivers_license on person.license_id=drivers_license.id
  inner join interview on person.id=interview.person_id
  inner join get_fit_now_member on person.id=get_fit_now_member.person_id
  where person.name like "Annabel%" and person.address_street_name = "Franklin Ave"
#+END_SRC

*Results*
| name           | transcript                                                                                                            |
|----------------+-----------------------------------------------------------------------------------------------------------------------|
| Annabel Miller | I saw the murder happen, and I recognized the killer from my gym when I was working out last week on January the 9th. |

*** Find Suspect query
    :PROPERTIES:
    :ID:       98C7155F-FC3C-4696-802B-057F641AB847
    :END:
#+BEGIN_SRC sql
  select 
    person.name,
    interview.transcript
  from get_fit_now_check_in
  inner join get_fit_now_member on get_fit_now_check_in.membership_id=get_fit_now_member.id
  inner join person on get_fit_now_member.person_id=person.id
  inner join drivers_license on drivers_license.id=person.license_id
  inner join interview on interview.person_id=person.id
  where
    get_fit_now_check_in.check_in_date = 20180109
    and get_fit_now_check_in.membership_id like "%48Z%"
    and drivers_license.plate_number like "%H42W%"
  order by get_fit_now_check_in.check_in_time desc
#+END_SRC

*Results*
| name          | transcript                                                                                                                                                                                                                                       |
|---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Jeremy Bowers | I was hired by a woman with a lot of money. I don't know her name but I know she's around 5'5" (65") or 5'7" (67"). She has red hair and she drives a Tesla Model S. I know that she attended the SQL Symphony Concert 3 times in December 2017. |

*** Find Real Culprit query
    :PROPERTIES:
    :ID:       AA46BA97-EC97-44FC-A590-3B31BF49463E
    :END:
#+BEGIN_SRC sql
  select
    person.name
  from drivers_license
  inner join person on person.license_id=drivers_license.id
  inner join facebook_event_checkin on facebook_event_checkin.person_id=person.id
  where
    height between 65 and 67
    and hair_color = "red"
    and car_make   = "Tesla"
    and car_model  = "Model S"
  limit 1
#+END_SRC

*Results*
| name             |
|------------------|
| Miranda Priestly |
* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/the-monad-analogy.org][The Monad Analogy]]
:PROPERTIES:
:RSS_PERMALINK: the-monad-analogy.html
:PUBDATE:  2019-10-29
:ID:       E1B4D688-FE60-438F-8826-3564D771BA9B
:END:
#+DESCRIPTION: This blog post will be discussing an analogy I use to depict what a monad is.

I've been using the Either and Option monads on various projects (in different runtimes) over the last couple of years. I like utilizing these monads as a way of handle errors from external systems. However, when it comes time to introducing someone new to these monads, I'm unable to describe what makes a monad a /monad/. However, after doing some research I found an analogy that encapsulates what I believe to be the spirit of what makes a monad a /monad/.

#+BEGIN_QUOTE
A monad is an abstract data type that allows programmers to chain complex, nondeterministic operations.
#+END_QUOTE

An abstract data type (ADT) is a kind of data type defined by it's behavior from the point of view from a user. Since an ADT is created from the user's point of view, it's internal presentation is hidden, thus enabling us to focus more on behavior than caring what data is being held (it could be a String, Integer, YourCustomObject, etc).

The ability to chain operations is the /functor/ attribute of monads. In category theory, a /functor/ is a something has enables /mappable/ behavior, such as iterating over a list of objects/strings/etc. To enable the chaining of /nondeterministic/ operations, two functions are found on monadic types: *return* and *bind*.

The *return* function /places/ a value into a monadic context, whereas the *bind* function /applies/ a function in a monadic context.

#+BEGIN_QUOTE
Note: Depending on the programming language or library that you are using, the *return* function will typically be represented through the /constructor/ method and *bind* may have a different name such as /flatMap/ or /map/.
#+END_QUOTE

Let's write an example of monads in action using [[https://kotlinlang.org/][Kotlin]] and a functional programming library called [[https://arrow-kt.io/docs/apidocs/arrow-core-data/arrow.core/-option/][Arrow]]. A quick real-world example is when I'm trying to eat a donut from my favorite donut place in the Upper West Side, [[https://www.dailyprovisionsnyc.com/menus/][Daily Provisions]]. Let's write a function that encapsulates the operation of getting a donut from Daily Provisions and, for simplicity, we'll choose the Option monad as its return type.

#+BEGIN_SRC kotlin
  fun getDonut(time: Date): Option<Donut> =
    if (donutsAreAvailable(time)) {
      Some(Donut())
    } else {
      None
    }
#+END_SRC

The above function could return either my donut or nothing depending on some external factor (time). When we pass a value into the Some function, we are effectively using the /return/ trait of a monadic type. The None function is a type of Option that returns...nothing! Next, do something with...something.

#+BEGIN_SRC kotlin
  val eatADonut = getDonut(timeBefore11am)
    .map { donut -> donut.eat() }
#+END_SRC

The above variable gets a donut and if a donut is found it will /map/, or /bind/, to a function that eats the donut. If the /getDonut/ returns a None, because I overslept and missed the opportunity to get a donut, then our /map/ method will never get called.
* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/building-a-blog-with-jekyll.org][Building a Blog with Jekyll]]
:PROPERTIES:
:RSS_PERMALINK: building-a-blog-with-jekyll.html
:PUBDATE:  2015-11-24
:ID:       0ED530CC-FDCD-42FC-AFE6-27471CDD4C3A
:END:
#+DESCRIPTION: In this blog post, I'll be discussing how to setup and build your own blog using Jekyll.

Many developers and designers have chosen to use [[https://jekyllrb.com/docs][Jekyll]], a static-page generator, for some of their web-based projects for its ease of use and simple design. In this post, we will be building a minimal blog with the help of Jekyll. This post is intended to be read by programming beginners who want to build a blog "the hacker way".

** What is Jekyll
   :PROPERTIES:
   :ID:       1DFB23C3-003A-435C-81B5-7467F61C6CEE
   :END:
Jekyll is an open-source static page generator written in [[https://www.ruby-lang.org][Ruby]], which can be used to build blogs, documentation, and other types of communication tools. Some features of Jekyll include:

text-based configuration
liquid templating for custom layouts
a plugin system
a [[https://en.wikipedia.org/wiki/Command-line_interface][cli]] tool

To see a full set of features Jekyll offers check out the project [[https://jekyllrb.com][home page]].

** Getting Started with Jekyll
   :PROPERTIES:
   :ID:       46E9FCF8-A314-4B3F-9090-37FBE236A464
   :END:
To get started with Jekyll, you will need [[https://www.ruby-lang.org/en/downloads][Ruby]] installed on your machine. To check if *ruby* is installed on your machine, run the following command in your CLI program:

#+BEGIN_SRC bash
  ruby -v
#+END_SRC

If *ruby* was not found on your machine, you can find installation notes [[https://www.ruby-lang.org/en/downloads][here]].

Next, let's install *jekyll* and *bundler*, a ruby gem for managing ruby project dependencies. Run the following command to install these gems:

#+BEGIN_SRC bash
  gem install bundler jekyll
#+END_SRC

** Generating a Blog
   :PROPERTIES:
   :ID:       F0AB46E9-33AC-4BA5-AF32-D63EE68C5FB2
   :END:
So far we have setup ruby and installed *jekyll*, next we will generate our blog. To generate the blog, run the following command:

#+BEGIN_SRC bash
  jekyll new my-first-blog
#+END_SRC

You'll see that a new directory called *my-first-blog* has been created and inside the directory contains some markdown and yaml files and a new directory called /_posts/. Jump into the newly created directory, by running the following command:

#+BEGIN_SRC bash
  cd my-first-blog
#+END_SRC

Next, we'll start up the blogging generation process by running the command:

#+BEGIN_QUOTE
*Quick Note* - We'll be running *bundle* before each of our *jekyll* commands, which is the "de-facto" *rubygem* dependency manager for the jekyll project. To learn more about *bundler* check out the following [[https://bundler.io/docs.html][link]].
#+END_QUOTE

#+BEGIN_SRC bash
  bundle exec jekyll serve
#+END_SRC

After a few moments, you should see a newly created folder called /_site/. Now, head over to your browser and type /http://localhost:4000/ and press enter. You should be seeing a boilerplate-filled web page such as the one below ↓.

[[file:./images/localhost-screenshot.png]]

** Editing the Blog
   :PROPERTIES:
   :ID:       094BFC43-DCF8-49C7-9885-D10EE599D597
   :END:
When your blog was created with the following files and folders were created:
*_posts* - folder containing your blog posts.
*_config.yml* - file for configuring project settings and global variables. 
*index.markdown* - your index file which relies on a home layout.
*about.markdown* - another markdown file which we can ignore for now. 
*Gemfile* - file for describing your *rubygems* dependencies.
*Gemfile.lock* - file for version controlling your *rubygems* dependency tree.  
*404.html* - file containing Not Found content.

When your blog was generated, a *_site* folder was created. Each time your blog is built, this *_site* folder will be regenerated with your latest changes to your blog. A common workflow when making changes to your blog is the ability to "watch" for changes.

Run the following command to watch for changes as your editing your blog:

#+BEGIN_SRC bash
  bundle exec jekyll serve -w
#+END_SRC

To see new changes to your blog, just change anything contained in *_posts* folder and your pages (such as *index.markdown* and *about.markdown*). 

** Writing new Blog Posts
   :PROPERTIES:
   :ID:       79B30544-F1D0-4539-8961-2132D675D0D1
   :END:
While the *bundle exec jekyll serve -w* command is running in the background, create a new file in *_posts* called *2018-11-24-my-new-blog-post.markdown*. Run the following command to create the new blog post:

#+BEGIN_SRC bash
  touch _posts/2018-11-24-my-new-blog-post.markdown
#+END_SRC

Next, jump into your favorite editor (mine happens to be [[https://www.vim.org][vim]] when I'm trying things out) to start editing this file. For Jekyll to understand our blog-post we'll need to give it some attributes the file's "header". Let's start with the following attributes to describe our new post content.

#+BEGIN_SRC markdown
  ---
  layout: post
  title:  "My New Blog Post!"
  date:   2018-11-24 00:00:00
  ---
#+END_SRC

Next, save the file and look at the output of your terminal window running the *jekyll serve -w* command. You should be seeing a newly added blog post called *my-new-blog-post*. Refreshing your browser should show your new blog post, however there is no content to view. That's because we need to write the rest of our blog post!

#+BEGIN_QUOTE
*Quick Note* - Before making any more changes to your new blog post, checkout the *_site/jekyll* folder to see that your new blog post has been generated and placed here. This *_site/jekyll* folder layout can be changed by updating your site configuration file *_config.yml* (permalink).
#+END_QUOTE

To continue writing your blog post, begin by writing beneath the last trip dashes and save. Check out your changes in the browser, edit, save, and repeat! For more information on the Markdown system supported by Jekyll, check out the following [[https://jekyllrb.com/docs/posts][link]].

** Conclusion
   :PROPERTIES:
   :ID:       EE0D7535-1FF3-4D7D-9DC1-92A70545C40F
   :END:
There is still much to explore in what Jekyll offers. I left out some features worth discussing such as [[https://jekyllrb.com/docs/themes][Theming]] and [[https://jekyllrb.com/docs/configuration/options][Site Configuration]] because Jekyll's [[https://jekyllrb.com/docs][documentation]] is just good enough.
* [[file:/Users/tjmaynes/workspace/tjmaynes/org-blog/content/posts/drawing-shapes-with-nao.org][Drawing Shapes with NAO]]
:PROPERTIES:
:RSS_PERMALINK: drawing-shapes-with-nao.html
:PUBDATE:  2015-05-09
:ID:       01AC42B0-C4B2-4500-8995-8944A2801BC9
:END:
#+DESCRIPTION: This blog post discusses how to "teach" the NAO robot to draw shapes that it "sees" using NAOqi (NAO SDK), Python and OpenCV.

This blog post discusses how to "teach" the NAO robot to draw shapes that it "sees" using NAOqi, Python and OpenCV. By the end of this blog post, you should have a better understanding of Canny Edge Detection and using Pixel Position interpolation to map a virual space to Forward Kinematics.

[[file:./images/nao-bot-1.png]]

** Background
   :PROPERTIES:
   :ID:       6BB6FD2F-C1A8-413C-88DB-56018233947A
   :END:
I just graduated with a Computer Science degree and wanted to share my Robotics class project that I'm proud of working on. Movies like [[https://www.imdb.com/title/tt0371746/][Iron Man]] and [[https://www.imdb.com/title/tt0103064][Terminator 2]] have inspired me to learn more about Computer Vision and Robotics. So, for my last semester of college, I was pretty excited to enroll in the Robotics course taught by [[http://www.cse.usf.edu/~yusun/][Dr. Yu Sun]].

Dr. Sun had some very expensive robots including a programmable industrial "arm", the NAO Robot and some homegrown robots. Roughly 70% of Dr. Sun's Robotic course grade came from a semester long group project. [[https://www.linkedin.com/in/tommylin1/][Tommy Lin]], my classmate, and I decided to team up on the course project. Tommy and I decided that we could learn and implement some Computer Vision and Robotics concepts together using the NAO Robot. For our project, we decided to write software to teach the NAO robot to draw shapes it "sees" using NAO SDK, Python and OpenCV.

** Project Design
   :PROPERTIES:
   :ID:       ED86AF56-5242-435B-8F54-38BBBB3AD1D6
   :END:
In order to program the NAO robot to draw basic shapes (seen through the camera sensor), some important design decisions needed to be examined at the beginning of this project. With the help of our course Teacher's Assistant, Caitrin Eaton, we decided these design pieces needed to include:

Setting up a workspace for the NAO robot to physcially work within.
Understanding and implementing an edge detection alogrithm to identify shapes.
Mapping and interpolating the detected shapes (as pixel points) to points in the NAO's workspace.
Debugging and testing how the NAO robot draws smooth lines.

*** Setting up the Physical Workspace
    :PROPERTIES:
    :ID:       ED71E09D-CF6C-47AF-A3A8-41799AA56F5A
    :END:
[[file:./images/nao-bot-2.png]]

We decided early on that we were going to use Forward Kinematics to move the NAO arm. Next, we came up with a general solution for describing lines and shapes in the form of a lookup table. The lookup table contains four coordinate positions (found in Figure 2) that make up a bounding area box area for the NAO to draw within. Since we are using the NAO robot's right arm, RArm, each one of the four coordinate positions are a list of six theta values/joint angles (/RShoulderPitch/, /RShoulderRoll/, /RElbowYaw/, /RElbowRoll/, /RWristYaw/, /RHand/).

Using this lookup table and the NAOqi *getAngles* function, we will enable the NAO robot to draw the shapes it sees within its workspace.

** Project Implementation
   :PROPERTIES:
   :ID:       A1FB7BF4-6EE0-42B9-8C03-63E4684FEC38
   :END:
*** Edge Detection
    :PROPERTIES:
    :ID:       F104B7A8-FA78-4686-8439-DF5DBE6658C7
    :END:
In order to implement edge detection using the NAO robot's camera, we first had PIL, Python Imaging Library. PIL allowed us to capture and save the image from the NAO's camera. Next, we read in the captured photo and used OpenCV's Canny Edge Detection function. The Canny Edge Detection algorithm is implemented in following order:

We read the image created by PIL, called *noognagnook.png*, from disk by OpenCV's imread function. The /imread/ function loads an image from file.
The loaded image is now converted to black and white using OpenCV's /cvtColor/ function. The /cvtColor/ function coverts an image from one color space to another.
The black and white image is now blurred using OpenCV's /GaussianBlur/ function. The black and white image is blurred because we do not want any extra noise or else that noise will show up as a line in the edge detection. The /GaussianBlur/ function blurs an image using a gaussian filter.
The black and white blurred image is passed to OpenCV's Canny function. The Canny function creates a mask of thresholds/bright lines that represent our shapes seen by the NAO robot.
The canny image is saved to a file for referencing/debugging.
The canny image is now passed to a function called /findContours/. The /findContours/ function returns a sets of Numpy arrays of pixel positions that make up each contour found in an image.
For each contour found, we take a smaller number of points that it will take to make that contour by using OpenCV's /approxPolyDP/. The /approxPolyDP/ function uses an alogrithm called [[https://en.wikipedia.org/wiki/Ramer–Douglas–Peucker_algorithm][Douglas-Peucker algorithm]] that will approximate a curve or a polygon with another curve/polygon with less vertices so that the distance between them is less or equal to the specified precision.
Allow the NAO robot to draw a shape using the approximate number of points found at a current contour that creates a polygon.

Lots of testing was done to make sure that we were using the approriate number of points and the correct points to draw from. Now that we have edge detection setup properly, we can now interpolate the detected pixel position to our workspace by using a lookup table and bilinear interpolation.

[[file:./images/nao-bot-3.png]]

*** Pixel Position Interpolation
    :PROPERTIES:
    :ID:       46D4DB4E-0A46-4024-97EE-064340129F08
    :END:
In order to interpolate the pixel positions found, we first had to setup a function called *lookup_table* to pass our desired pixel positions to and return a list of lists of theta values.

The lookup table created is a matrix of size: 640 columns by 460 rows (pixels from the camera sensor generated image). Next, we had to manually get our four bounding box points by setting the NAO's right arm stiffness to zero and calling /getAngles/, from the NAOqi API, on the specified grid coordinate. The four specified grid coordinates included: grid[0][0], grid[640][0], grid[0][460], and grid[640][460]. In each of the four specified grid coordinates, there contains a list of six theta values that represent how the NAO's right arm is to get the specified position in its workspace. Once we have our setup our lookup table, we can now move on to mapping the pixels to the specific grid positions via bilinear interpolation.

The purpose of interpolating the detected pixel positions is to get the theta values needed for the NAO to travel to each desired pixel position needed to draw the desired shape. We will interpolate the theta values desired by comparing their respective pixel position to the four bounded pixel positions in the workspace. We can achieve this by creating a function called /bilinear_interpolation/, whose algorithm is based on a mathematical formula called Bilinear Interpolation. Using the following algorithm we can create a list of pixel position theta values called path for the NAO to travel to:

We create a list called path. This list will be used as the final output of the list of lists of theta values needed for the NAO robot to draw a desired shape to.
Create a variable called *go_back_to_start* which will record the first pixel position. This pixel position will be used as the last point for the NAO to draw to.
For each set of points and for each point of those set of points, append the result calculated by the /bilinear_interpolation/ function. The /bilinear_interpolation/ function is passed three arguments: x-coordinate of the current point, y-coordinate of the current point, and a list of the bounding positions (which contains a list of theta values).
The /bilinear_interpolation/ function will return a list of six theta values that will specify how the NAO will motion the current pixel position.
This loop continues until we have all the desired lists of pixel position theta values.
The lookup_table function returns a path list containing a list of all the interpolated pixel position values (each of which contains a list of six theta values).

Now that we have a path for the NAO robot, we can use this path to create the shape desired. We can create the shape desired by simply looping through each point in the path list. Next, we pass the effector name (/RArm/), the current point in the path, and a specified speed (0.3 seconds) to the /setAngles/ function (from the NAOqi function). The setAngles function will take a set of theta values and a specified effector (like the right arm) and perform a motion trajectory to the desired position via the set of theta values.

[[file:./images/nao-bot-4.png]]

*** Testing and Feedback
    :PROPERTIES:
    :ID:       266863CA-0D80-45BF-AF2B-C86044ADAD87
    :END:
To effectively start testing the NAO's ability to draw shapes, we had to make sure the following was working accordingly: image processing was return the correct pixel positions desired; the bilinear interpolation of the each (correct) pixel position found was calculated correctly; and the motion trajectory of the path generated was working properly. Assuming that the previously mentioned list was in fact working properly, then we would be able to debug and test for smoother line drawing. However, over the course of the semester, we encountered practically every issue that could occur in the previously mentioned list. Specifically, a majority of the tests completed were based on debugging issues we encountered from image processing and interpolation issues from our motion trajectory algorithm. Also, issues such as how the NAO gripped the marker affected how well the seen shapes were drawn.

We were having issues with choosing the right set of pixel positions for the NAO robot to interpolate from and draw from. Particulary, we had issues with the data structure created by OpenCV to hold the pixel position data returned as an Numpy array of lists of coordinate values. When we tried to have NAO draw based on the pixel positions that we were reading in, the NAO would draw all sorts of various lines that were not coherient to the image being seen. We started debugging this issue by working backwards on the issue by making sure that the bilinear interpolation function was working properly. Using print statements, we found that the points being read in were properly. This was proven by cross checking the returned list of interpolated pixel position theta values list were within the bounds of the theta values of each corner pixel position. Next, we tested the points that were being passed into the lookup_table function. At first the points being passed were the entire set of contours found in the image. This is not necessary a problem, unless the NAO picks up other contours (like noise) found in the image. After minor tweaking the area that the NAO looks at, we were able to get a much cleaner set of contours that made up a single shape for the NAO to draw from. Finally, we tested which of the set of points we wanted to draw. At first, we used the entire set of contours found in the image, however ultimately we decided not to draw 2000 positions in the NAO's workspace. Instead, we decided to use the approximate number of points that will make a polygon found at a current contour. This proved to work as needed since we were able to draw four points in the NAO's workspace as opposed to 2000 points.

The issues that caused the motion trajectory problems were from not fully understanding the ALMotion API (part of NAOqi API). Early on, we started by using the positionInterpolation function, which moves end-effectors to given positions and orientations over a period of time. This was a mistake from the very start since we were trying to apply angles to a position interpolation. This issue stemed from the fact that we did not fully understand the NAOqi API. Next, we tried to use the angleInterpolation function, which interpolates one or multiple joints to a target angle or along timed trajectories. This also proved to be a complicated mistake that we never fully realized why the issues occured. Basically, no matter what the shape was seen, the NAO would make the same right arm motions. Finally, we figured out a much simplier solution by looping through each point in list of points and calling the setAngles function to draw to each position. This proved to be the easiest to setup and ultimately made the best looking shapes. We were able to prove this by feeding the setAngles function the bounding coordinates found, which should have the NAO robot travel to the specific bounding positions. The setAngles function caused the NAO to smoothly go to each corner position in its bounding box.

Finally, there were problems with how well the NAO robot was drawing the shapes that it had seen. The main reason behind these issues was that the NAO had a difficult time gripping the marker that was being used to draw the shape seen. Other reasons also include the friction and force, from the speed of the NAO's setAngle execution, caused the marker to move around slightly creating curved lines. We were able to fix a majority of these problems by making sure the NAO's grip on the marker was more stable by adding another rubberband around its hand.

#+CAPTION: Figure 6 - Final video of the NAO's journey to discovering the power of drawing shapes.
[[youtube-video:1_-gUEW5GuY]]

** Conclusion
   :PROPERTIES:
   :ID:       4F1BA70C-80D0-4AC4-AA83-09C254D87C53
   :END:
Given a semesters worth of time, we were able to have the NAO robot draw a somewhat decent interpretation of the shape it sees. We learned quite a bit about robotics from this project including properly setting up motion trajectories, understanding and using motion interpolation and bilinear interpolation for pixel positions found to the workspace, and how to decide the various approaches that can be used to solve a robotics related problem. If we had more time, we would liked to have the NAO robot draw different shapes/objects dependent in the NAO's workspace. Also, we would have liked to be able to tweak the motion trajectory algorithm in order to have smoother lines drawn. This could have potentially been solved by using a marker that had a prismatic joint for its tip, so that there would be little friction between the paper and the marker (no jerkiness between each angle position drawn). Also, it could have been pretty cool to if the NAO robot could draw, based on our current codebase, other objects like trees, cars, houses, etc.

We would like to thank Caitrin Eaton for all her help and support for making our progress possible. Caitrin helped us understand bilinear interpolation, how to define the NAO's workspace, and did an incredible job explaining other things along the way. Also, we would like to thank Garfield Huang for his help teaching us about the various ways the NAO operates. Finally, we would like to thank Dr. Yu Sun for allowing us to work on this project this semester! Dr. Sun's research can be found [[http://www.cse.usf.edu/~yusun/][here]]!

** Sources
   :PROPERTIES:
   :ID:       933C74CF-EB57-4C51-8BF6-2984F46E0175
   :END:
[[http://doc.aldebaran.com/1-14/naoqi/motion/control-joint-api.html#ALMotionProxy::setAngles__AL::ALValueCR.AL::ALValueCR.floatCR][Aldebaran - setAngles function]]
[[http://doc.aldebaran.com/1-14/naoqi/motion/control-joint-api.html#ALMotionProxy::getAngles__AL::ALValueCR.bCR][Aldebaran - getAngles function]]
[[http://en.wikipedia.org/wiki/Bilinear_interpolation][Wikipedia - Bilinear Interpolation]]
[[http://stackoverflow.com/questions/8661537/how-to-perform-bilinear-interpolation-in-python][StackOverflow - How to perform bilinear interpolation in Python]]
[[http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html][OpenCV - Canny Edge Detector]]
[[http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours][OpenCV - findContours]]
[[http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=cv2.approxpolydp#cv2.approxPolyDP][OpenCV - approxPolyDP]]
